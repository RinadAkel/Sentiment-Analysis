{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RinadAkel/Sentiment-Analysis/blob/main/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjIkQKLsstGy"
      },
      "source": [
        "© 2021 Zaka AI, Inc. All Rights Reserved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q24hSxA4tA6n"
      },
      "source": [
        "# Text classification\n",
        "\n",
        "Text classification is one of the important tasks of text mining\n",
        "\n",
        "![alt text](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1535125878/NLTK3_zwbdgg.png)\n",
        "\n",
        "In this notebook, we will perform Sentiment Analysis on IMDB movies reviews. Sentiment Analysis is the art of extracting people's opinion from digital text. We will use a regression model from Scikit-Learn able to predict the sentiment given a movie review. \n",
        "\n",
        "We will use [the IMDB movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/), which consists of 50,000 movies review (50% are positive, 50% are negative)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O1jA8byt4bV"
      },
      "source": [
        "* [Numpy](http://www.numpy.org/) — a package for scientific computing.\n",
        "* [Pandas](https://pandas.pydata.org/) — a library providing high-performance, easy-to-use data structures and data analysis tools for the Python\n",
        "* [Matplotlib](https://matplotlib.org/) — a package for plotting & visualizations.\n",
        "* [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
        "* [NLTK](http://www.nltk.org/) — a platform to work with natural language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDGnUVl78wgp"
      },
      "source": [
        "##Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_58dUtw81T0"
      },
      "source": [
        "### Importing the libraries and necessary dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRN4WqkltlB5"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "\n",
        "# download Punkt Sentence Tokenizer\n",
        "nltk.download('punkt')\n",
        "# download stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A4PCqft855C"
      },
      "source": [
        "### Loading the dataset in our directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c48UYWDcg3hR"
      },
      "source": [
        "# download IMDB dataset\n",
        "!wget \"https://raw.githubusercontent.com/javaidnabi31/Word-Embeddding-Sentiment-Classification/master/movie_data.csv\" -O \"movie_data.csv\"  \n",
        "\n",
        "# list files in current directory\n",
        "!ls -lah  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3RK3JCT8-CC"
      },
      "source": [
        "###Reading the dataset file and getting info on it\n",
        "Using pandas to read the csv file and displaying the first 10 rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0A5QhDlteWj"
      },
      "source": [
        "# the path to the IMDB dataset\n",
        "dataset_path = 'movie_data.csv'  \n",
        "\n",
        "# read file (dataset) into our program using pandas\n",
        "data = pd.read_csv(dataset_path) \n",
        "\n",
        "# display first 10 rows\n",
        "data.head(10)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaKHERKQ94NE"
      },
      "source": [
        "Getting info on our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQVx6AhqhAiB"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAkD49hD97Ja"
      },
      "source": [
        "A balanced dataset in sentiment analysis is a dataset which holds an equal amount of positive sentiment data and negative sentiment data, meaning 50% of the data is positive and 50% is negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q12nMYY5vPhn"
      },
      "source": [
        "# check if dataset is balanced (number of positive sentiment = number of negative sentiment) \n",
        "# by plotting the different classes\n",
        "data.sentiment.value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4uAuueIwKkS"
      },
      "source": [
        "## Text cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCxs0pSovUOa"
      },
      "source": [
        "print(data.review[10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAvczEBgxUWl"
      },
      "source": [
        "Let's define a function that would clean each movie review (sentence)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKKIsHqZwRJR"
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "english_stopwords = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def clean_review(text):\n",
        "  # convert to lower case\n",
        "  text = text.lower()\n",
        "\n",
        "  # remove none alphabetic characters\n",
        "  text = re.sub(r'[^a-z]', ' ', text)\n",
        "\n",
        "  # stem words \n",
        "  # split into words\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # stemming of words\n",
        "  stemmed = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "  text = ' '.join(stemmed)\n",
        "\n",
        "  # remove stopwords\n",
        "  text = ' '.join([word for word in text.split() if word not in english_stopwords])\n",
        "\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fcElPMS-GO6"
      },
      "source": [
        "And try it out on an instance of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4Bn3r1wzvwR"
      },
      "source": [
        "print(data['review'][1])\n",
        "print(clean_review(data['review'][1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVrxiOd2-VSp"
      },
      "source": [
        "And now clean the entire dataset reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kHxWkPTz5eA"
      },
      "source": [
        "# apply to all dataset\n",
        "data['clean_review'] = data['review'].apply(clean_review)\n",
        "data.head() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkVqSSzu2Ax8"
      },
      "source": [
        "## Split dataset for training and testing\n",
        "We will split our data into two subsets: a 50% subset will be used for training the model for prediction and the remaining 50% will be used for evaluating or testing its performance. The random state ensures reproducibility of the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPHlwVS71brN"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "X = data['clean_review'].values\n",
        "y = data['sentiment'].values\n",
        "\n",
        "# Split data into 50% training & 50% test\n",
        "# let's all use a random state of 42 for example to ensure having the same split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz23g0nD2nhN"
      },
      "source": [
        "## Feature extraction with Bag of Words\n",
        "In this section, let's apply the Bag of Words method to learn the vocabulary of our text and with it transform our training input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_B0vrn-2sON"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# define a CountVectorizer (with binary=True and max_features=10000)\n",
        "vectorizer = CountVectorizer(binary=True, max_features=10000)  \n",
        "\n",
        "# learn the vocabulary of all tokens in our training dataset\n",
        "vectorizer.fit(x_train)  \n",
        "\n",
        "# transform x_train to bag of words\n",
        "x_train_bow = vectorizer.transform(x_train)  \n",
        "x_test_bow = vectorizer.transform(x_test)   \n",
        "\n",
        "print(x_train_bow.shape, y_train.shape)\n",
        "print(x_test_bow.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtLaJfuw4060"
      },
      "source": [
        "## Classification\n",
        "Our data is ready for classification. Let's use [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mS51YGO4hfv"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# define the LogisticRegression classifier\n",
        "model = LogisticRegression()\n",
        "\n",
        "# train the classifier on the training data\n",
        "model.fit(x_train_bow, y_train)\n",
        "\n",
        "# get the mean accuracy on the training data\n",
        "acc_train = model.score(x_train_bow, y_train)  \n",
        "\n",
        "print('Training Accuracy:', acc_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GD6JSa6-m6g"
      },
      "source": [
        "Evaluating the performance of our model through its accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJnyoqO5NyE"
      },
      "source": [
        "# Evaluate model with test data\n",
        "\n",
        "acc_test = model.score(x_test_bow, y_test)  \n",
        "print('Test Accuracy:', acc_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh5927-d6Gq4"
      },
      "source": [
        "## Let's use the model to predict!\n",
        "To do so, let's create a predict function which takes as argument our model and the bag of words vectorizer together with a review on which it would predict the sentiment. This review should be cleaned with the `clean_review` function we built, transformed by bag of words and then used for prediction with `model.predict()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6kxkZ5m55Ii"
      },
      "source": [
        "# define predict function\n",
        "def predict(model, vectorizer, review):\n",
        "    review = clean_review(review)\n",
        "    review_bow = vectorizer.transform([review])\n",
        "    return model.predict(review_bow)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1_7g8s1-qed"
      },
      "source": [
        "And let's try it out on an example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z6WCl916flD"
      },
      "source": [
        "review = 'The movie was great!!!'\n",
        "predict(model, vectorizer, review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_147s0SGjjN0"
      },
      "source": [
        "review2 = 'the film was not Bad'\n",
        "predict(model, vectorizer, review2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}